---
title: "Tokenization"
author: "CHINDU"
output:
  pdf_document: default
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Text preprocessing

Tokenization is the act of breaking up a sequence of strings into pieces such as words, keywords, phrases, symbols and other elements called tokens. Tokens can be individual words, phrases or even whole sentences. In the process of tokenization, some characters like punctuation marks are discarded.

## Reading data and setting the correct structure

```{r}
library(widyr)
library(tm)
library(tidytext)
library(dplyr)
library(SnowballC)
datax<- read.csv("Womens Clothing ECommerce Reviews.csv")
str(datax)
```
## We need to change our text variable from a factor to a character for analysis
```{r}
# Subset data to only the fields requiried
data<-datax %>% select(Clothing.ID,Review.Text)
data$Review.Text<-as.character((data$Review.Text))
data$Clothing.ID<-as.factor(data$Clothing.ID)
str(data)
```

# Tokenization 

## Split the data into sentences (This is usually useful for long text documents)
```{r}

data %>%
  unnest_tokens(output = "sentences", input = Review.Text, token = "sentences")%>%
count(Clothing.ID,sentences,sort=TRUE)

```

## Split the data into words (This is a common approach)
```{r}

data %>%
  unnest_tokens(output = "word", input = Review.Text, token = "words")%>%
  count(Clothing.ID,word,sort=TRUE)
```


## Split the data using regular expressions
```{r}
data %>%
  unnest_tokens(output = "regexsplit", input = Review.Text,
                token = "regex", pattern = "\\.") %>%
  count(Clothing.ID,regexsplit,sort=TRUE)
```


## Filter data identify sentenses which mentions love regardless of capital letter

```{r,results='asis'}
Filtered_data<-data %>%
  unnest_tokens (output= "Love",Review.Text,
                 token="regex",pattern="(?i)love") %>%
  #to skip first token
  slice(2:n())
```

## 2. Normalization

Normalization generally reers to a series of related tasks meant to pull all text on a level playing field. Example converting all text to lower casse, removing punctuations, converting number to their word equivalents etc. Normalization puts all words on equal footing and allows processing to proceed uniformly.

```{r}
# First tokenize by words
clothes <- data %>%
  unnest_tokens(word, Review.Text) 

# Print the word frequencies
clothes %>%
  count(word, sort = TRUE)
```

The top words are all stopwords and this is not going to be useful for the analysis.

## Removing stop words
```{r}
# Remove stop words, using `stop_words` from tidytext
clothes<-clothes %>%
  anti_join(stop_words)
clothes %>%
  count(word, sort = TRUE)
```
##Custom stop words (to remove words that you feel is not useful in this analysis)
lets remove the word "online"
```{r}
custom<- add_row(stop_words,word= "online",lexicon="custom")
Custom_clothes<- clothes %>%
  anti_join(custom)
```

## Stemming
Stemming is the process of eliminating affixes from a word in order to obtain a word stem.
```{r}
# Perform stemming 
stemmed_clothes <- Custom_clothes %>%
  mutate(word = wordStem(word))

# Print the old word frequencies 
Custom_clothes %>%
  count(word, sort = TRUE)

# Print the stemmed word frequencies
stemmed_clothes %>%
  count(word, sort = TRUE)
```
Notice how the top words changed when we applied stemming

## Lemmatization
```{r}
library(textstem)
lemmatize_words(Custom_clothes)%>%
  count(word, sort = TRUE)
```

In this data, lemmatization did not make any changes to the data.

Lets explore the difference between lemmatization and stemming 
```{r}
dw <- c('driver', 'drive', 'drove', 'driven', 'drives', 'driving')

stem_words(dw)

lemmatize_words(dw)

bw <- c('are', 'am', 'being', 'been', 'be')

stem_words(bw)

lemmatize_words(bw)
```

Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma. Linguistic processing for stemming or lemmatization is often done by an additional plug-in component to the indexing process, and a number of such components exist, both commercial and open-source.


here is another example

```{r}

y <- c(
    "Stemming refers to a crude heuristic process that chops off the ends of words.",
     "Lemmatization refers to doing things properly with the use of a vocabulary and morphological analysis of words"
)

stem_strings(y)
lemmatize_strings(y)

```

